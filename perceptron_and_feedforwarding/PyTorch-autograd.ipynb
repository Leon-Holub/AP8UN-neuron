{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "PyTorch je framework pro strojové učení založený na knihovně Torch, používaný pro aplikace umělé inteligence, například v počítačovém vidění či zpracování přirozené mluvy. Původně byl vyvinutý Meta AI, nyní je součástí Linux Foundation. Je to bezplatný a open-source software vydaný pod upravenou licencí BSD.\n",
    "\n",
    "Je optimalizovaný pro práci s tenzory a grafy. Umožňuje provádět výpočty na GPU.\n",
    "    \n",
    "Základní vlastnosti:\n",
    "1. PyTorch tensor (velice podobný Numpy ndarray).\n",
    "1. Přímé, necyklické grafy (Direct acyclic graphs) pro modelování modelů umělé inteligence a hlubokého učení.\n",
    "1. Dynamicky sestavované grafy (např. Tenforflow pracuje se statickými grafy) je možné měnit vstupy i operace za běhu.\n",
    "1. Pythoní knihovna, pro \"pythonistu\" jednoduchá na pochopení a použití (vs Keras, Tensorflow).\n",
    "\n",
    "Instalace pomocí Anaconda\n",
    "\n",
    "```conda install pytorch torchvision cpuonly -c pytorch```\n",
    "\n",
    "Instalace pomocí pip\n",
    "\n",
    "```pip3 install torch torchvision```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "!pip3 install numpy matplotlib torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# Základní vlastnosti tenzoru\n",
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operace s Tenzory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standartní slicing jako v numpy\n",
    "tensor = torch.ones(4, 4)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Přesun tensoru na GPU, pokud je k dispozici\n",
    "if torch.cuda.is_available():\n",
    "  tensor = tensor.to('cuda')\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranzpozice tenzoru\n",
    "t_inverse = tensor.T \n",
    "print(tensor)\n",
    "print(t_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sčítání tenzorů\n",
    "t0 = tensor + tensor\n",
    "print(t0)\n",
    "t1 = tensor + 5\n",
    "print(t1)\n",
    "t2 = tensor.add(5)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Násobení tenzorů jednotlivé prvky\n",
    "t3 = tensor * tensor\n",
    "print(t3)\n",
    "t4 = tensor.mul(tensor)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maticové násobení \n",
    "t5 = tensor @ tensor.T\n",
    "print(t5)\n",
    "t6 = tensor.matmul(tensor.T)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spojování tenzorů\n",
    "t7 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suma tenzoru\n",
    "t8 = tensor.sum()\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum a minimum\n",
    "tmax = tensor.max()\n",
    "print(tmax)\n",
    "tmin = tensor.min()\n",
    "print(tmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-place operace\n",
    "print(tensor)\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most do NumPy (Bringe with NumPy)\n",
    "Torch Tensor na CPU a NumPy Array  mohou sdílet paměť -> změna jednoho změní i druhý."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Tenzor do NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}, type: {type(t)}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}, type: {type(n)}\")\n",
    "# změna tenzoru se odrazí i v NumPy array\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy array do PyTorch Tenzoru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "print(f\"n: {n}\")\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t: {t}\")\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trénovaní neuronové sítě\n",
    "Trénování neuronové sítě probíhá ve dvou stále se opakujících krocích:\n",
    "\n",
    "**Forward Propagation:** Neuronová síť vrací svůj odhad správného výstupu. Data prochází jednotlivými vrstvami/funkcemi modelu a je vypočítána výstupní hodnota.\n",
    "\n",
    "**Backward Propagation:** Během backpropagation, model upravuje své parametry podle chyby na výstupu. Toto je prováděno pomocí postupného návratu od výstupu ke vstupu, sledují se změny gradientu a parametry funkcí modelu jsou optimalizovány pomocí gradient descent. Pro vizualizaci backpropagation doporučuji třeba [video od 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd - jednoduchá ukázka\n",
    "### Analytické řešení v Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import math\n",
    "\n",
    "a = np.linspace(0., 2. * math.pi, 25)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.sin(a)\n",
    "plt.plot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ukazka vypoctu\n",
    "c = 2 * b\n",
    "d = c + 1\n",
    "out = d.sum()\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zde je analytické řešení pro náš případ:\n",
    "\n",
    "1. b = np.sin(a) - derivace sin(a) je cos(a).\n",
    "1. c = 2 * b - derivace 2 * sin(a) je 2 * cos(a), protože derivace konstanty je konstanta a derivace sin(a) je cos(a).\n",
    "1. d = c + 1 - derivace 2 * cos(a) + 1 je stále 2 * cos(a), protože derivace konstanty je 0.\n",
    "1. out = d.sum() - zde se výstupy z předchozího kroku sumarizují, ale protože chceme gradient každého prvku a vzhledem k out, musíme se vrátit k tomu, že gradient každého prvku a vzhledem k jeho příspěvku v out je 2 * cos(a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vypocet gradientu\n",
    "gradient_a = 2 * np.cos(a)\n",
    "gradient_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a, gradient_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ukazka vypoctu\n",
    "c = 2 * b\n",
    "d = c + 1\n",
    "out = d.sum()\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autograd\n",
    "out.backward()\n",
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Gradient Descent (Gradientní sestup) je optimalizační algoritmus používaný v strojovém učení k minimalizaci chyby výstupu modelu (en: cost function), která kvantifikuje, jak \"daleko\" je model od ideálního řešení. Základní myšlenka gradient descent spočívá v iterativním upravování parametrů modelu (například vah v neuronové síti) s cílem postupně snížit chybu modelu. Výpočet probíhá iterativně:\n",
    "\n",
    "1. Výpočet Gradientu:\n",
    "Gradient je vektor, který udává směr nejstrmějšího stoupání funkce. V kontextu optimalizace chceme najít opačný směr, tedy směr nejstrmějšího klesání, aby se minimalizovala funkce nákladů.\n",
    "Pro každý parametr modelu (např. váhu) se vypočítá parciální derivace cost function vzhledem k tomuto parametru, což indikuje, jak malá změna v parametru ovlivní celkovou hodnotu cost function.\n",
    "1. Aktualizace Parametrů  \n",
    "Parametry modelu se aktualizují podle vzorce:  \n",
    "$$ \\theta_{n+1} = \\theta_n - \\eta \\cdot \\nabla_\\theta J(\\theta)$$  \n",
    "kde:  \n",
    "$\\theta$ jsou parametry modelu, které se mají optimalizovat.  \n",
    "$\\eta$ je rychlost učení (learning rate), což je kladný skalár určující velikost kroku při aktualizaci parametrů.  \n",
    "$\\nabla_\\theta $ je gradient funkce nákladů $J$ vzhledem k parametrům $\\theta$, který udává směr nejstrmějšího stoupání funkce nákladů.  \n",
    "Při aktualizaci se pohybujeme v opačném směru k nalezení minima funkce nákladů."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definice modelu\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # 2 vstupy -> 1 výstup\n",
    "        self.aktivace = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.aktivace(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializace modelu, ztrátové funkce a optimalizátoru\n",
    "model = Perceptron()\n",
    "criterion = nn.BCELoss() # Binární cross-entropy ztráta pro klasifikační úlohy\n",
    "SDGoptimizer = optim.SGD(model.parameters(), lr=0.1) # Stochastic gradient descent optimizer\n",
    "\n",
    "# Trénovací data\n",
    "X = torch.randn(5, 2, requires_grad=False)\n",
    "Y = torch.randn(5, 1, requires_grad=False)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vahy perceptronu\n",
    "print(model.linear.weight)\n",
    "print(model.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vypocet vystupu modelu\n",
    "prediction = model(X[0])\n",
    "print(prediction)\n",
    "Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vzorova loss function\n",
    "loss = (Y[0] - prediction).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vypocet gradientu\n",
    "loss.backward()\n",
    "print(model.linear.weight)\n",
    "print(model.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uprava vah pomoci SDG\n",
    "SDGoptimizer.step()\n",
    "print(model.linear.weight)\n",
    "print(model.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kumulace gradientu\n",
    "for i in range(5):\n",
    "    prediction = model(X[i])\n",
    "    loss = (Y[i] - prediction).pow(2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "print(model.linear.weight)\n",
    "print(model.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vynulovani gradientu\n",
    "SDGoptimizer.zero_grad(set_to_none=False)\n",
    "print(model.linear.weight)\n",
    "print(model.linear.weight.grad)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Na konci se může vrátit na start s upravenými váhami.\n",
    "\n",
    "LOSS = BCE (Binary Cross Entropy)\n",
    "\n",
    "## Entropy\n",
    "Entropy je míra nejistoty nebo překvapení. V teorii informací je entropie pravděpodobnostní míra, která měří množství informace obsažené v pravděpodobnostním rozdělení. V kontextu strojového učení je entropie často používána jako ztrátová funkce pro klasifikační modely.\n",
    "výpočet informační entropie:\n",
    "čím výšší pravděpodobnost, tím nižší entropie.\n",
    "Příklad:\n",
    "pravděpodobnost výhry Čr je 99%\n",
    "pravděpodobnost výhry USA je 1%\n",
    "-99% * log2(99%) - 1% * log2(1%) = 0.08\n",
    "\n",
    "Využijeme sigmoid aktivační fci.\n",
    "y = our -> O or 1 -> y = k\n",
    "y^ = predicted -> 0 or 1 -> y^ = p\n",
    "\n",
    "B(k|p) = p^k * (1-p)^(1-k) -> if p -> k = 1; if 1-p -> k = 0\n",
    "P(yi|y^i) = y^i * (1 - y^i)^(1 - y^i) -> BCE\n",
    "\n",
    "P = suma ale s násobením (P(yi|y^i))\n",
    "výpočet -> pi^yi * (1 - pi)^(1 - yi) -> BCE (p = y^)\n",
    "\n",
    "Moc náročné na paměť cheme SUM\n",
    "\n",
    "P = sum\n",
    "\n",
    "## Binary Cross Entropy\n",
    "- to předtím bylo useless\n",
    "\n",
    "BCE = -1/n * sum(yi * log(y^i) + (1 - yi) * log(1 - y^i))\n",
    "\n",
    "## Gradient Descent\n",
    "- chyba = loss\n",
    "- lr = learning rate\n",
    "- W = váhy\n",
    "- dL/dW = derivace chyby podle vah\n",
    "- dL = derivace chyby\n",
    "- dWold = derivace původní váhy\n",
    "- Wnew = nová váha\n",
    "- Wold = původní váha\n",
    "\n",
    "Wnew = Wold - lr * dL/dWold\n",
    "\n",
    "\n",
    "## NonBinary\n",
    "- používá aktivační fci softmax\n",
    "- \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
