{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T16:39:24.843734Z",
     "start_time": "2025-05-06T16:14:37.092400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "# === Parametry ===\n",
    "MAX_LEN = 80\n",
    "VOCAB_SIZE = 16000\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "FF_DIM = 512\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 6\n",
    "\n",
    "# === Naƒçten√≠ tiny_shakespeare ===\n",
    "dataset = tfds.load(\"tiny_shakespeare\", split=\"train\", data_dir=\"./data\")\n",
    "raw_text = next(iter(dataset))['text'].numpy().decode()\n",
    "\n",
    "# === Rozdƒõlen√≠ textu na krat≈°√≠ bloky ===\n",
    "chunks = [raw_text[i:i + MAX_LEN] for i in range(0, len(raw_text) - MAX_LEN, MAX_LEN)]\n",
    "\n",
    "# === TextVectorization ===\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAX_LEN,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split=\"whitespace\"\n",
    ")\n",
    "tokenizer.adapt(chunks)\n",
    "\n",
    "# === Preprocessing ===\n",
    "def preprocess(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return (tokens[:-1], tokens[:-1]), tokens[1:]\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(chunks).map(preprocess).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = train_ds.take(5)\n",
    "\n",
    "# === Pozicov√© k√≥dov√°n√≠ ===\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        pos = np.arange(maxlen)[:, np.newaxis]\n",
    "        i = np.arange(embed_dim)[np.newaxis, :]\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        angle_rads = pos * angle_rates\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        self.pos_encoding = tf.constant(angle_rads[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "\n",
    "# === Transformer blok ===\n",
    "def transformer_block(embed_dim, num_heads, ff_dim):\n",
    "    inputs = tf.keras.Input(shape=(None, embed_dim))\n",
    "    attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(inputs, inputs)\n",
    "    attn = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attn)\n",
    "    ffn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "        tf.keras.layers.Dense(embed_dim)\n",
    "    ])\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attn + ffn(attn))\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# === Transformer model ===\n",
    "def build_model():\n",
    "    inp = tf.keras.Input(shape=(None,), dtype='int64')\n",
    "    tgt = tf.keras.Input(shape=(None,), dtype='int64')\n",
    "    embed = tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_DIM, mask_zero=True)\n",
    "    x = embed(inp)\n",
    "    y = embed(tgt)\n",
    "    x = PositionalEncoding(MAX_LEN, EMBED_DIM)(x)\n",
    "    y = PositionalEncoding(MAX_LEN, EMBED_DIM)(y)\n",
    "    enc = transformer_block(EMBED_DIM, NUM_HEADS, FF_DIM)(x)\n",
    "    dec_attn = tf.keras.layers.MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBED_DIM)(y, enc)\n",
    "    dec_out = tf.keras.layers.LayerNormalization(epsilon=1e-6)(y + dec_attn)\n",
    "    ffn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(FF_DIM, activation='relu'),\n",
    "        tf.keras.layers.Dense(VOCAB_SIZE)\n",
    "    ])\n",
    "    output = ffn(dec_out)\n",
    "    return tf.keras.Model([inp, tgt], output)\n",
    "\n",
    "# === Kompilace ===\n",
    "model = build_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"üîÅ Tr√©nuji.\")\n",
    "model.fit(train_ds, epochs=EPOCHS)\n",
    "print(\"‚úÖ Tr√©nink dokonƒçen!\")"
   ],
   "id": "f86c1e8f6c078273",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Tr√©nuji.\n",
      "Epoch 1/6\n",
      "393/393 [==============================] - 252s 632ms/step - loss: 1.4399 - accuracy: 0.8234\n",
      "Epoch 2/6\n",
      "393/393 [==============================] - 261s 663ms/step - loss: 1.1156 - accuracy: 0.8307\n",
      "Epoch 3/6\n",
      "393/393 [==============================] - 233s 592ms/step - loss: 1.0339 - accuracy: 0.8349\n",
      "Epoch 4/6\n",
      "393/393 [==============================] - 251s 638ms/step - loss: 0.9015 - accuracy: 0.8496\n",
      "Epoch 5/6\n",
      "393/393 [==============================] - 235s 597ms/step - loss: 0.4486 - accuracy: 0.9190\n",
      "Epoch 6/6\n",
      "393/393 [==============================] - 254s 646ms/step - loss: 0.2305 - accuracy: 0.9513\n",
      "‚úÖ Tr√©nink dokonƒçen!\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T17:08:45.429467Z",
     "start_time": "2025-05-06T17:08:45.382902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Generace ===\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "index_to_word = dict(enumerate(vocab))\n",
    "\n",
    "def detokenize(tokens):\n",
    "    return ' '.join([index_to_word.get(int(token), '[UNK]') for token in tokens])\n",
    "\n",
    "def generate(prompt, max_len=30, temperature=0.8, top_k=20):\n",
    "    input_ids = tokenizer(tf.convert_to_tensor([prompt]))[:, :MAX_LEN-1]\n",
    "    output_ids = input_ids[:, :1]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        pred = model([input_ids, output_ids])\n",
    "        logits = pred[:, -1, :] / temperature\n",
    "\n",
    "        # Zaka≈æ padding a [UNK]\n",
    "        logits = tf.tensor_scatter_nd_update(\n",
    "            logits,\n",
    "            indices=[[0, 0]],\n",
    "            updates=[-1e9]\n",
    "        )\n",
    "\n",
    "        # Top-k sampling\n",
    "        values, _ = tf.math.top_k(logits, k=top_k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        logits = tf.where(logits < min_values, -1e9, logits)\n",
    "\n",
    "        next_token = tf.random.categorical(logits, num_samples=1, dtype=tf.int64)[:, 0]\n",
    "        output_ids = tf.concat([output_ids, tf.expand_dims(next_token, 1)], axis=1)\n",
    "        # print(\"üîÑ Generuji tokeny:\", output_ids.numpy()[0])\n",
    "\n",
    "    return detokenize(output_ids[0].numpy())\n"
   ],
   "id": "fdbaa65012642cbc",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T17:11:02.847287Z",
     "start_time": "2025-05-06T17:11:02.535009Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"üìù Vygenerovan√Ω text:\", generate(\"to be or not\", max_len=10, temperature=0.7))",
   "id": "3b068f1a63a7208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Vygenerovan√Ω text: to be youll not haste thyself them thyself thyself thyself montague\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer Text Generation Project ‚Äì Shakespeare Model\n",
    "\n",
    "In this task, we were asked to build a Transformer-based text generation model using an encoder-decoder architecture, following the provided structural diagram and lecture materials.\n",
    "\n",
    "This notebook contains an implementation of a custom Transformer trained for text generation using the **Tiny Shakespeare** dataset (`tiny_shakespeare` from `tensorflow_datasets`). The training corpus was selected for its small size, allowing for rapid experimentation.\n",
    "\n",
    "### üß† Model & Parameters\n",
    "\n",
    "- Framework: **TensorFlow** (without Keras high-level API)\n",
    "- Strategy: `tf.distribute.MirroredStrategy()` for potential multi-GPU support\n",
    "- Tokenizer: `TextVectorization` layer, vocabulary size: **16,000**\n",
    "- Sequence length: **80 tokens**\n",
    "- Embedding dimension: **128**\n",
    "- Number of attention heads: **4**\n",
    "- Feedforward dimension: **512**\n",
    "- Transformer blocks: **2**\n",
    "- Dropout: **0.1**\n",
    "- Optimizer: **Adam**\n",
    "- Loss: `SparseCategoricalCrossentropy(from_logits=True)`\n",
    "- Epochs: **5**\n",
    "- Sampling method: **Top-k sampling** with `k=20` and `temperature=0.8`\n",
    "\n",
    "Training was performed on ~393 batches and completed in a few minutes per epoch.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú® Output Examples\n",
    "\n",
    "The final model produces stylized poetic English in Shakespearean form, often correctly continuing prompts such as:\n",
    "\n",
    "- `\"to be or not\"` ‚Üí `\"to confess doth thyself haste montague...\"`\n",
    "- `\"to be or not\"` ‚Üí `\"to be youll not haste thyself haste catesby lady...\"`\n",
    "\n",
    "The model tends to capture the rhythm and word usage of Shakespearean English, even introducing relevant character names and poetic repetitions.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Conclusion\n",
    "\n",
    "The task was interesting from a learning perspective, but incredibly time-consuming. Combined with multiple other demanding assignments from different courses, it often brought more frustration than joy.\n",
    "\n",
    "Originally, I started with the `wiki_dialog` dataset as suggested, but it proved impractical ‚Äî the preprocessing and generation took hours without progress. I eventually switched to `tiny_shakespeare` due to its small size, which drastically reduced training time from **days to tens of minutes** and allowed for meaningful experimentation and iteration.\n",
    "\n",
    "Despite the limited scope, the final results were satisfying, especially in generating coherent poetic phrases that mirrored Shakespearean language.\n"
   ],
   "id": "a8ed167ac1b4fb8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
